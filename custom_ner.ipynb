{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Note:\" data-toc-modified-id=\"Note:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Note:</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-existing spacy model\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the pipeline component\n",
    "ner = nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data example\n",
    "# (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DATA = [\n",
    "#               (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
    "#               (\"I booked a Lamborghini yesterday\", {\"entities\": [(11, 22, \"ORG\")]}),\n",
    "#               (\"I recently ordered a book from Amazon\", {\"entities\": [(31,37, \"ORG\")]})\n",
    "#               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(text, ents):\n",
    "    \"\"\"For single line of text\"\"\"\n",
    "    mapped_ents = []\n",
    "    for ent in ents:\n",
    "        ent_idx = re.search(ent[0], text).span()\n",
    "        mapped_ents.append((ent_idx[0], ent_idx[1], ent[1]))\n",
    "    return (text, {\"entities\": mapped_ents})\n",
    "\n",
    "def create_bulk_training_data(data):\n",
    "    \"\"\"For list of text\"\"\"\n",
    "    return [create_training_data(item[0], item[1]) for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I recently ordered a book from Amazon',\n",
       " {'entities': [(21, 25, 'PRODUCT'), (31, 37, 'ORG')]})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_training_data(\n",
    "    text = \"I recently ordered a book from Amazon\",\n",
    "    ents = [\n",
    "        (\"book\", \"PRODUCT\"), \n",
    "        (\"Amazon\", \"ORG\")\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\n",
    "        \"Walmart is a leading e-commerce company in the U.S\",\n",
    "        [\n",
    "            (\"e-commerce\", \"SERVICE_TYPE\"),\n",
    "            (\"U.S\", \"LOC\")\n",
    "        ]),\n",
    "    (\n",
    "        \"I booked a Lamborghini yesterday\",\n",
    "        [\n",
    "            (\"Lamborghini\", \"ORG\")\n",
    "            ]),\n",
    "    (\n",
    "        \"I recently ordered a book from Amazon\",\n",
    "        [\n",
    "            (\"Amazon\", \"ORG\")\n",
    "            ])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Walmart is a leading e-commerce company in the U.S',\n",
       "  {'entities': [(0, 7, 'ORG')]}),\n",
       " ('I booked a Lamborghini yesterday', {'entities': [(11, 22, 'ORG')]}),\n",
       " ('I recently ordered a book from Amazon', {'entities': [(31, 37, 'ORG')]})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA = create_bulk_training_data(data=data)\n",
    "TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding labels to the `ner`\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before you train, remember that apart from ner , the model has other pipeline components. These components should not get affected in training.\n",
    "\n",
    "So, disable the other pipeline components through `nlp.disable_pipes()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ner']\n"
     ]
    }
   ],
   "source": [
    "# Disable pipeline components you dont need to change\n",
    "pipe_exceptions = ['tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'transformer', 'tok2vec']\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "print(unaffected_pipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from spacy.util import minibatch\n",
    "from spacy.training import Example\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 2:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 3:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 4:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 5:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 6:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 7:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 8:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 9:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 10:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 11:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 12:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 13:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 14:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 15:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 16:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 17:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 18:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 19:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n",
      "Loss after epoch 20:  {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# To train an ner model, the model has to be looped over the example for sufficient number of iterations (atleast 20).\n",
    "EPOCHS = 20\n",
    "optimizer = nlp.create_optimizer()\n",
    "\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Before every iteration itâ€™s a good practice to shuffle the examples randomly through random.shuffle() function. \n",
    "        # This will ensure the model does not make generalizations based on the order of the examples.\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        \n",
    "        losses={}\n",
    "        \n",
    "        # The training data is usually passed in batches. \n",
    "        # Use the minibatch() function of spaCy over the training data that will return you data in batches. \n",
    "        # The minibatch function takes size parameter to denote the batch size.\n",
    "        for batch in minibatch(TRAIN_DATA, size=1):\n",
    "            for text, annotations in batch:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                # For each iteration , the model or ner is updated through the nlp.update() command.\n",
    "                nlp.update([example], drop=0.35, sgd=optimizer, losses=losses)\n",
    "        print(f\"Loss after epoch {epoch+1}: \", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "* Parameters of **`nlp.update()`** are :\n",
    "\n",
    "    * **docs:** \n",
    "    This expects a batch of texts as input. You can pass each batch to the zip method, which will return you batches of text and annotations.\n",
    "\n",
    "    * **golds:** \n",
    "    You can pass the annotations we got through zip method here\n",
    "\n",
    "    * **drop:** \n",
    "    This represents the dropout rate.\n",
    "    \n",
    "    * **losses:** \n",
    "    A Dictionary to hold the losses against each pipeline component. Create an empty dictionary and pass it here.\n",
    "\n",
    "\n",
    "\n",
    "* At each word, the **`update()`** it makes a prediction. It then consults the annotations to check if the prediction is right. If it isnâ€™t , it adjusts the weights so that the correct action will score higher next time.\n",
    "\n",
    "\n",
    "All of the training is done within the context of the nlp model with disabled pipeline, to prevent the other components from being involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"custom_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = nlp.from_disk(\"custom_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_doc = nlp1(\"Today I drove a Lamborghini and ordered a book from Amazon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Today\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " I drove a \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lamborghini\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and ordered a book from \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(ner_doc, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today DATE\n",
      "Lamborghini ORG\n",
      "Amazon ORG\n"
     ]
    }
   ],
   "source": [
    "for word in ner_doc.ents:\n",
    "    print(word.text, word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_link</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>articles</th>\n",
       "      <th>scraped_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.livemint.com/companies/news/it-sta...</td>\n",
       "      <td>2023-01-22T22:58:58+05:30</td>\n",
       "      <td>IT, startups mayâ€Š cutâ€Š upâ€Što 20,000â€Š jobsâ€Š in ...</td>\n",
       "      <td>layoffs,job cuts,Indian IT,startups,employees,...</td>\n",
       "      <td>Pressure from investors, an expected global re...</td>\n",
       "      <td>2023-01-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.livemint.com/companies/news/rbitoa...</td>\n",
       "      <td>2023-01-22T22:39:03+05:30</td>\n",
       "      <td>RBIâ€Š mayâ€Š appeal â€ŠBombay HCâ€Š order onâ€Š Yesâ€Š Ba...</td>\n",
       "      <td>RBI,Bombay high court,Yes Bank,additional tier...</td>\n",
       "      <td>Bombay high court ruling could have a huge sec...</td>\n",
       "      <td>2023-01-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.livemint.com/companies/people/we-w...</td>\n",
       "      <td>2023-01-22T22:36:50+05:30</td>\n",
       "      <td>â€˜We want to maximize HNI, retail interest in A...</td>\n",
       "      <td>Adani Enterprises,fpo,sale,adani cfo,retail in...</td>\n",
       "      <td>Adani group has been running investor outreach...</td>\n",
       "      <td>2023-01-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_link  \\\n",
       "0  https://www.livemint.com/companies/news/it-sta...   \n",
       "1  https://www.livemint.com/companies/news/rbitoa...   \n",
       "2  https://www.livemint.com/companies/people/we-w...   \n",
       "\n",
       "                    pub_date  \\\n",
       "0  2023-01-22T22:58:58+05:30   \n",
       "1  2023-01-22T22:39:03+05:30   \n",
       "2  2023-01-22T22:36:50+05:30   \n",
       "\n",
       "                                               title  \\\n",
       "0  IT, startups mayâ€Š cutâ€Š upâ€Što 20,000â€Š jobsâ€Š in ...   \n",
       "1  RBIâ€Š mayâ€Š appeal â€ŠBombay HCâ€Š order onâ€Š Yesâ€Š Ba...   \n",
       "2  â€˜We want to maximize HNI, retail interest in A...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  layoffs,job cuts,Indian IT,startups,employees,...   \n",
       "1  RBI,Bombay high court,Yes Bank,additional tier...   \n",
       "2  Adani Enterprises,fpo,sale,adani cfo,retail in...   \n",
       "\n",
       "                                            articles scraped_date  \n",
       "0  Pressure from investors, an expected global re...   2023-01-23  \n",
       "1  Bombay high court ruling could have a huge sec...   2023-01-23  \n",
       "2  Adani group has been running investor outreach...   2023-01-23  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('livemint_2023-01-23.json', lines=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pressure from investors, an expected global recession and the domino impact of global layoffs have resulted in staff cuts  Indiaâ€™s IT and startup sectors may lay off 15,000 to 20,000 employees in the next six months, battling slowing demand after the hiring frenzy of the last two years inflated salary costs.  Indiaâ€™s IT and startup sectors may lay off 15,000 to 20,000 employees in the next six months, battling slowing demand after the hiring frenzy of the last two years inflated salary costs.  Recruitment consultants expect fewer hiring mandates in the months ahead and have decided not to enter new businesses for now. Recruitment consultants expect fewer hiring mandates in the months ahead and have decided not to enter new businesses for now. However, even as some IT and startup companies will shed staff to manage costs, others within the same sectors are hiring, too. â€œWe expect about 20,000 layoffs over the next few quarters. Over the last year, companies faced the fear of missing out on talent hiring and recruited in large numbers and paid them many folds more than inflation and market standards,\" said Lohit Bhatia, president of workforce management for recruitment firm Quess Corp.  The cost of maintaining that talent has begun to hurt IT firms and startups, Bhatia added. In FY23 so far, the two sectors have laid off more than 20,000 employees, including 1,400 in the first two weeks of January. Many IT product and services firms, edtechs and fintechs are struggling after the over-hiring of pandemic years, when they snapped up talent at steep salaries assuming continued growth. Today, pressure from investors, an expected global recession and the domino impact of global layoffs have combined to result in staff cuts. Last week, Google fired 12,000 employees, joining tech giants such as Meta, Amazon and Microsoft, which have laid off 11,000, 18,000 and 11,000 workers, respectively. â€œFifteen thousand employees will get retrenched over the next six months, but this will be the last leg of the recession. However, there are skilled employees who are getting absorbed into the industry. Attrition in many of the product or IT service firms are upwards of 15%, which means despite layoffs, there is room for lateral hirings,\" said Kamal Karanth, co-founder of Xpheno, which specializes in technology and startup hiring. IT services companies such as Tata Consultancy Services Ltd, Infosys Ltd, Wipro Ltd and HCL Tech face high employee costs, even though overall headcount has fallen.  The impact is mostly on junior and middle-level employees, but search firms in charge of senior hiring in tech and startup sectors said leadership hiring had slowed as well. â€œThere are a couple of cases where the decision-making is getting delayed, but thatâ€™s about it. In fact, as a side effect of this retrenchment news, clients have raised their expectations, imagining that a lot of talent is now available,\" said Amit Agarwal, managing director, Singapore and India, Stanton Chase. â€œHowever, the matter of fact is companies are not letting go of their key talent or high performers at the top. So, the battle for leadership only intensifies,\" he added. Rituparna Chakraborty, co-founder and executive director of TeamLease Services, expects employers to remain cautious for at least the first quarter of the new fiscal. â€œIT hiring has gone down, and there is tepidness in the market,\" Chakraborty said.\n"
     ]
    }
   ],
   "source": [
    "x = df['articles'][0]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pressure from investors, an expected global recession and the domino impact of global layoffs have resulted in staff cuts  Indiaâ€™s IT and startup sectors may lay off 15,000 to 20,000 employees in the next six months, battling slowing demand after the hiring frenzy of the last two years inflated salary costs'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1bd3846165642f7a3fb0edb58a0363cf7277e1bcdd1557dc0bbb8da756cd0121"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
